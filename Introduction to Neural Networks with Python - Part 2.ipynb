{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T11:50:38.549278Z",
     "start_time": "2017-08-10T11:50:38.196868Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Introduction to Neural Networks with Python - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Review from Part 1\n",
    "\n",
    "### 1. Artificial Neural Networks\n",
    "\n",
    "An artificial neural network, or ANN, is __composed of a set of neurons, or perceptrons__, that are connected in a __hierarchical order__. \n",
    "\n",
    "An example of 3-layered ANN.\n",
    "![ANN](figs/ann.png)\n",
    "\n",
    "### 2. Perceptrons\n",
    "\n",
    "A perceptron is a node that accepts a __fixed arbitrary number of inputs__, and gives a __single output__.\n",
    "\n",
    "![Perceptron](figs/perceptron.png)\n",
    "\n",
    "$ output = \\alpha(w \\cdot x + b) $, where $ \\alpha $ refers to the __activation function__\n",
    "\n",
    "### 3. Activation (aka Transfer) Functions \n",
    "\n",
    "An activation function yields the numerical output of a perceptron. \n",
    "\n",
    "* __Binary classification__: Used to decide whether a neuron will fire or not (step function is sufficient).\n",
    "* __Multiclass classification__: Used to associate probabilities on each classes.\n",
    "\n",
    "Example presented was __sigmoid function__.\n",
    "\n",
    "![Sigmoid function](figs/sigmoid_eq.png)\n",
    "![Sigmoid graph](figs/sigmoid_plot.png)\n",
    "\n",
    "### 4. Cost (aka Objective / Loss) Functions\n",
    "\n",
    "Measure of our \"happiness\" with the outcome. We'll want to minimize this cost function.\n",
    "\n",
    "* __High loss__: Poor classifier\n",
    "* __Low loss__: Good classifier\n",
    "\n",
    "Example presented was __Mean Squared Error (MSE)__.\n",
    "\n",
    "$ C(w, b) = \\frac{1}{n} \\sum_x (y_p(x) - y_t)^2 $\n",
    "\n",
    "### 5. Optimization Algorithms\n",
    "\n",
    "Algorithms that aim to reduce the cost function.\n",
    "\n",
    "Example presented was __Gradient Descent__.\n",
    "\n",
    "Given the possible outputs of a cost function of a 2-input perceptron is the graph below.\n",
    "\n",
    "![Cost graph](figs/cost_graph.png)\n",
    "\n",
    "![Gradient Descent](figs/grad_descent_eq.png)\n",
    "\n",
    "$ \\eta $ (a hyperparameter) is the learning rate or how large the step of the gradient descent will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Multilayer Perceptron (MLP)\n",
    "\n",
    "Last meeting, we've played with a perceptron and tackled primary processes done in ANN. Often, we want to make our perceptrons more robust (or complicated) by adding more layers of these perceptrons to accommodate complexities brought by our training data  -- __Multilayer Perceptron__.\n",
    "\n",
    ">The predictive capability of neural networks comes from the hierarchical or multi-layered structure of the networks. The data structure can pick out (learn to represent) features at different scales or resolutions and combine them into higher-order features. For example from lines, to collections of lines to shapes ([Brownlee, J.](http://machinelearningmastery.com/neural-networks-crash-course/)).\n",
    "\n",
    "The figure below shows an example of MLP with 6 input neurons, 2 hidden layers (4,3), and 1 output neuron.\n",
    "\n",
    "<img src=\"figs/mlp.png\" alt=\"MLP\" style=\"width: 70%;\"/>\n",
    "\n",
    "\n",
    "Parts:\n",
    "* Input layer: where you encode your input values e.g. image pixel values. Number of input neurons in the input layer is then straightforward -- number of input values.\n",
    "* Output layer: contains your output neurons. In binary classification, can be designed to either have 1 or 2 neurons. In multiclass classification, the number of output neurons is the number of classes.\n",
    "* __Hidden layer/s__: neither your input nor output. Designing is not as straightforward -- can be an art."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Creating a 2-layer Neural Network\n",
    "\n",
    "Credits for the original code to Stanford [CS231n](http://cs231n.github.io/neural-networks-case-study/) and notebook annotations to ThinkingMachines [Tutorial](https://github.com/thinkingmachines/deeplearningworkshop.git)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T11:50:39.034303Z",
     "start_time": "2017-08-10T11:50:38.550971Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Initializing the given values for a Spiral Dataset\n",
    "\n",
    "Goal: Given a pair of x and y coordinates, determine which 'spiral arm' belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T11:50:39.040057Z",
     "start_time": "2017-08-10T11:50:39.035885Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "N = 100 # points per class\n",
    "D = 2 # dimensionality at 2 so we can eyeball it\n",
    "K = 3 # number of classes\n",
    "\n",
    "# X will contain the features / predictors\n",
    "# y will contain the labels or \"answers\"\n",
    "X = np.zeros((N*K, D)) # generate an empty matrix to hold X features\n",
    "y = np.zeros(N*K, dtype='uint8') # generate an empty vector to hold y labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### [Quick Note] Dimensions are crucial during computations\n",
    "\n",
    "We'll be doing a lot of matrix operations on the following steps. With this, it's worthwhile to have a short review on some linear algebra operations.\n",
    "\n",
    "* __Matrix multiplication__: It's important that the __column__ of the first matrix is of same size with the __row__ or the second matrix. Operation is not commutative, AxB != BxA.\n",
    "\n",
    "An example matrix multiplication of 1x3 matrix to 3x3 matrix.\n",
    "\n",
    "<img src=\"figs/mat_mult.png\" alt=\"Matrix Multiplication\" style=\"width: 50%;\"/>\n",
    "\n",
    "* __Matrix addition__: As long as the matrices are of the __same dimension__. :)\n",
    "\n",
    "An example matrix addition of two 2x2 matrices.\n",
    "\n",
    "<img src=\"figs/mat_add.png\" alt=\"Matrix Addition\" style=\"width: 40%;\"/>\n",
    "\n",
    "### ** Where do these take place in ANN? **\n",
    "\n",
    "An example illustration\n",
    "\n",
    "<img src=\"figs/dim_illus.png\" alt=\"Dimensions in ANN\" style=\"width: 70%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### [Quick Question] what are the dimensions of X and y?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Generating spiral dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T11:50:39.047789Z",
     "start_time": "2017-08-10T11:50:39.041788Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# for 3 classes, evenly generates spiral arms\n",
    "for j in range(K):\n",
    "    ix = range(N*j, N*(j+1))\n",
    "    r = np.linspace(0.0,1,N) #radius\n",
    "    t = np.linspace(j*4, (j+1)*4, N) + np.random.randn(N)*0.2 # theta\n",
    "    X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "    y[ix] = j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Let's visualize this. Setting S=20 (size of points) so that the color/label differences are more visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T11:50:39.356689Z",
     "start_time": "2017-08-10T11:50:39.049738Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=y, s=20, cmap=plt.cm.Spectral)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Training a Linear Classifier\n",
    "\n",
    "Let's start by training a a simple y = WX + b linear classifer on this dataset. We need to compute some Weights (W) and a bias vector (b) for all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T11:50:39.365541Z",
     "start_time": "2017-08-10T11:50:39.360531Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# random initialization of starting parameters\n",
    "# it's best to randomly initialize at a small value. \n",
    "# how many parameters should this linear classifier have? remember there are K output classes, and 2 features per observation.\n",
    "\n",
    "W = 0.01 * np.random.randn(D,K)\n",
    "b = np.zeros((1,K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### [TODO] Print out the shapes and values of W and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T11:50:39.378808Z",
     "start_time": "2017-08-10T11:50:39.369372Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# tip: to get the shape, use [list_var].shape\n",
    "\n",
    "# print(\"W: shape: {}, values: {}\".format(__fill this__, __fill this__))\n",
    "# print(\"b: shape: {}, values: {}\".format(__fill this__, __fill this__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T11:50:39.396488Z",
     "start_time": "2017-08-10T11:50:39.393322Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Here are some hyperparameters that we're not going to worry about too much right now\n",
    "\n",
    "learning_rate = 1e-0 # the step size in the descent\n",
    "reg = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### [TODO] Assign scores to variable 'scores'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T11:50:39.402015Z",
     "start_time": "2017-08-10T11:50:39.398660Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Translate 'scores = W (dot) X + b' to code\n",
    "# Tip: Check dimensions of variables prior the operations\n",
    "\n",
    "# scores = \n",
    "\n",
    "# Show the scores for the 51th input\n",
    "# print(scores[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### [Quick Question] What is the dimension of scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### [Quick Question] Aren't the values for the scores a bit hard to interpret? Is there a way to calibrate the scores to somehow fit to probability scores?\n",
    "\n",
    "### Answer: Softmax classifier\n",
    "\n",
    "Softmax classifier takes a vector of scores and squashes it between 0 and 1 that sum up to 1. It computes the __probability__ that an input belongs to a class.\n",
    "\n",
    "<img src=\"figs/softmax.png\" alt=\"Softmax\" style=\"width: 30%;\"/>\n",
    "\n",
    "where $ s = f(x_i, W) = W \\cdot X_i + b $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T11:50:39.418764Z",
     "start_time": "2017-08-10T11:50:39.410793Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's try softmax classifier out!\n",
    "\n",
    "num_examples = X.shape[0]\n",
    "\n",
    "exp_scores = np.exp(scores)\n",
    "probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "# Let's look at one example to verify the softmax transform\n",
    "print(\"Scores: {}\".format(scores[50])) ## The original scores\n",
    "print(\"Class probabilities: {}\".format(probs[50])) ## Softmax Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### [Quick Question] Can we then 'derive' our cost function using these probability scores? \n",
    "But.. In cost, don't we want a lower value? Probabilities have it in reverse - the higher the value is, the better.\n",
    "\n",
    "### Answer: Yes, we can. \n",
    "We'll just have to add '-log' to our softmax classifier to make it behave like the cost function we've discussed, i.e. lower is better.\n",
    "\n",
    "<img src=\"figs/log_cost.png\" alt=\"Log Softmax\" style=\"width: 40%;\"/>\n",
    "\n",
    "### [Quick Question] Why -log??\n",
    "Math magic: __High__ for probabilities close to 0, __low__ for probabilities close to 1. That's what we want!\n",
    "\n",
    "<img src=\"figs/neg_log.png\" alt=\"Negative Log\" style=\"width: 40%;\"/>\n",
    "\n",
    "\n",
    "### [Quick Question] Okay, now that we have our desired loss behavior. Don't we apply it not just for an input, but for the whole (training) set?\n",
    "\n",
    "### Answer: Yes. \n",
    "\n",
    "The full loss over the training set is shown below.\n",
    "\n",
    "<img src=\"figs/full_loss.png\" alt=\"Full Loss\" style=\"width: 40%;\"/>\n",
    "\n",
    "### [Quick Question] What is the regularization loss for?\n",
    "\n",
    "### Answer: We add it to the full loss function to favor simpler parameters W\n",
    "\n",
    "Suppose we find a W such that L is very small. __But__, there can exist other matrices W such that L is very small, but makes parameters W bigger -- more complicated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T11:50:39.425633Z",
     "start_time": "2017-08-10T11:50:39.420458Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "correct_logprobs = -np.log(probs[range(num_examples),y]) \n",
    "\n",
    "# data loss is L1 loss plus regularization loss\n",
    "data_loss = np.sum(correct_logprobs)/num_examples\n",
    "reg_loss = 0.5*reg*np.sum(W*W)\n",
    "loss = data_loss + reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T11:50:39.431813Z",
     "start_time": "2017-08-10T11:50:39.427260Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# this gets the gradient of the scores\n",
    "\n",
    "# class probabilities minus - divided by num_examples \n",
    "dscores = probs\n",
    "dscores[range(num_examples),y] -= 1\n",
    "dscores /= num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T11:50:39.438477Z",
     "start_time": "2017-08-10T11:50:39.433606Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# this backpropages the gradient into W and b\n",
    "dW = np.dot(X.T, dscores) # don't forget to transpose! otherwise, you'll be forwarding the gradient\n",
    "dW += 0.5*W # regularization gradient\n",
    "\n",
    "db = np.sum(dscores, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Updating the Parameters\n",
    "\n",
    "We update the parameters W and B in the direction of the negative gradient in order to decrease the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T11:50:39.451045Z",
     "start_time": "2017-08-10T11:50:39.441183Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# this updates the W and b parameters\n",
    "W += -learning_rate * dW\n",
    "b += -learning_rate * db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Full Code for the Training the Linear Softmax Classifier\n",
    "\n",
    "Using gradient descent method for optimization.\n",
    "\n",
    "Using L1 for loss funtion.\n",
    "\n",
    "This ought to converge to a loss of around 0.78 after 150 iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T11:50:39.540853Z",
     "start_time": "2017-08-10T11:50:39.455004Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# initialize parameters randomly\n",
    "W = 0.01 * np.random.randn(D,K)\n",
    "b = np.zeros((1,K))\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 1e-0\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = X.shape[0]\n",
    "\n",
    "# evaluated for 200 steps\n",
    "for i in range(200):\n",
    "  # evaluate class scores, [N x K]\n",
    "  scores = np.dot(X, W) + b \n",
    "  \n",
    "  # compute the class probabilities\n",
    "  exp_scores = np.exp(scores)\n",
    "  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "  \n",
    "  # compute the loss: average cross-entropy loss and regularization\n",
    "  corect_logprobs = -np.log(probs[range(num_examples),y])\n",
    "  data_loss = np.sum(corect_logprobs)/num_examples\n",
    "  reg_loss = 0.5*reg*np.sum(W*W)\n",
    "  loss = data_loss + reg_loss\n",
    "  \n",
    "  # for every 10 iterations print the loss\n",
    "  if i % 10 == 0:\n",
    "    print(\"iteration {}: loss {}\".format(i, loss))\n",
    "  \n",
    "  # compute the gradient on scores\n",
    "  dscores = probs\n",
    "  dscores[range(num_examples),y] -= 1\n",
    "  dscores /= num_examples\n",
    "  \n",
    "  # backpropate the gradient to the parameters (W,b)\n",
    "  dW = np.dot(X.T, dscores)\n",
    "  db = np.sum(dscores, axis=0, keepdims=True)\n",
    "  \n",
    "  dW += reg*W # regularization gradient\n",
    "  \n",
    "  # perform a parameter update\n",
    "  W += -step_size * dW\n",
    "  b += -step_size * db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Evaluating the Training Accuracy\n",
    "\n",
    "The training accuracy here ought to be at around 0.5\n",
    "\n",
    "This is better than change for 3 classes, where the expected accuracy of randomly selecting one of out 3 labels is 0.33. But not that much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T11:50:39.547604Z",
     "start_time": "2017-08-10T11:50:39.542681Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "scores = np.dot(X, W) + b\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print('training accuracy: {}'.format(np.mean(predicted_class == y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T11:50:39.882637Z",
     "start_time": "2017-08-10T11:50:39.549352Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# plot the resulting classifier\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = np.dot(np.c_[xx.ravel(), yy.ravel()], W) + b\n",
    "Z = np.argmax(Z, axis=1)\n",
    "Z = Z.reshape(xx.shape)\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Training a 2 Layer Neural Network\n",
    "\n",
    "Let's see what kind of improvement we'll get with adding a single hidden layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T11:50:39.893394Z",
     "start_time": "2017-08-10T11:50:39.884407Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# init parameters \n",
    "\n",
    "N = 100 # points per class\n",
    "D = 2 # dimensionality at 2 so we can eyeball it\n",
    "K = 3 # number of classes\n",
    "\n",
    "np.random.seed(100) # so we all have the same numbers\n",
    "\n",
    "h = 100 # size of hidden layer. a hyperparam in itself.\n",
    "\n",
    "W = 0.01 * np.random.randn(D,h)\n",
    "b = np.zeros((1,h))\n",
    "\n",
    "W2 = 0.01 * np.random.randn(h,K)\n",
    "b2 = np.zeros((1,K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Let's use a ReLU activation function. See how we're passing the scores from one layer into the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T11:50:39.901471Z",
     "start_time": "2017-08-10T11:50:39.897065Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "hidden_layer = np.maximum(0, np.dot(X, W) + b) \n",
    "scores = np.dot(hidden_layer, W2) + b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "The loss computation and the dscores gradient computation remain the same. The major difference lies in the the chaining backpropagation of the dscores all the way back up to the parameters W and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T11:50:39.914739Z",
     "start_time": "2017-08-10T11:50:39.905579Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# backpropate the gradient to the parameters of the hidden layer\n",
    "dW2 = np.dot(hidden_layer.T, dscores)\n",
    "db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "# gradient of the outputs of the hidden layer (the local gradient)\n",
    "dhidden = np.dot(dscores, W2.T)\n",
    "\n",
    "# backprop through the ReLU function\n",
    "dhidden[hidden_layer <= 0] = 0\n",
    "\n",
    "# back right into the parameters W and b\n",
    "dW = np.dot(X.T, dhidden)\n",
    "db = np.sum(dhidden, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Full Code for Training the 2 Layer NN with ReLU activation\n",
    "\n",
    "Very similar to the linear classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T11:50:47.508550Z",
     "start_time": "2017-08-10T11:50:39.919014Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# initialize parameters randomly\n",
    "\n",
    "np.random.seed(100) # so we all have the same numbers\n",
    "\n",
    "h = 100 # size of hidden layer\n",
    "W = 0.01 * np.random.randn(D,h)\n",
    "b = np.zeros((1,h))\n",
    "W2 = 0.01 * np.random.randn(h,K)\n",
    "b2 = np.zeros((1,K))\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 1e-0\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "\n",
    "# optimization: gradient descent loop\n",
    "num_examples = X.shape[0]\n",
    "for i in range(10000):\n",
    "  # feed forward \n",
    "  \n",
    "  # evaluate class scores, [N x K]\n",
    "  hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation\n",
    "  scores = np.dot(hidden_layer, W2) + b2\n",
    "  \n",
    "  # compute the class probabilities\n",
    "  exp_scores = np.exp(scores)\n",
    "  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "  \n",
    "  # compute the loss: average cross-entropy loss and regularization\n",
    "  corect_logprobs = -np.log(probs[range(num_examples),y])\n",
    "  data_loss = np.sum(corect_logprobs)/num_examples\n",
    "  reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n",
    "  loss = data_loss + reg_loss\n",
    "  if i % 1000 == 0:\n",
    "    print(\"iteration {}: loss {}\".format(i, loss))\n",
    "  \n",
    "  # backprop \n",
    "  \n",
    "  # compute the gradient on scores\n",
    "  dscores = probs\n",
    "  dscores[range(num_examples),y] -= 1\n",
    "  dscores /= num_examples\n",
    "  \n",
    "  # backpropate the gradient to the parameters\n",
    "  # first backprop into parameters W2 and b2\n",
    "  dW2 = np.dot(hidden_layer.T, dscores)\n",
    "  db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "  # next backprop into hidden layer\n",
    "  dhidden = np.dot(dscores, W2.T)\n",
    "  # backprop the ReLU non-linearity\n",
    "  dhidden[hidden_layer <= 0] = 0\n",
    "  # finally into W,b\n",
    "  dW = np.dot(X.T, dhidden)\n",
    "  db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "  \n",
    "  # add regularization gradient contribution\n",
    "  dW2 += reg * W2\n",
    "  dW += reg * W\n",
    "  \n",
    "  # perform a parameter update\n",
    "  W += -step_size * dW\n",
    "  b += -step_size * db\n",
    "  W2 += -step_size * dW2\n",
    "  b2 += -step_size * db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Evaluating the Training Set Accuracy\n",
    "\n",
    "This should be around 0.98, which is hugely better than the 0.50 we were getting from the linear classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T11:50:47.585943Z",
     "start_time": "2017-08-10T11:50:47.510183Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "hidden_layer = np.maximum(0, np.dot(X, W) + b)\n",
    "scores = np.dot(hidden_layer, W2) + b2\n",
    "\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print('training accuracy: {}'.format(np.mean(predicted_class == y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Let's visualize this to get a more dramatic sense of just how good the split is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-10T11:50:48.006870Z",
     "start_time": "2017-08-10T11:50:47.587697Z"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# plot the resulting classifier\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = np.dot(np.maximum(0, np.dot(np.c_[xx.ravel(), yy.ravel()], W) + b), W2) + b2\n",
    "Z = np.argmax(Z, axis=1)\n",
    "Z = Z.reshape(xx.shape)\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "#fig.savefig('spiral_net.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Now Try:\n",
    "\n",
    "* Generating another couple of arms in the dataset by modifying K and testing the classifiers on the new dataset\n",
    "* Change the hyperparameters, see how many iterations it takes to for the classifier to roughly converge.\n",
    "* Change the learning rate to a really high number, observe the iterations to convergence.\n",
    "* Change the activation function, loss scoring, or optimization method\n",
    "* Add a 3rd hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
