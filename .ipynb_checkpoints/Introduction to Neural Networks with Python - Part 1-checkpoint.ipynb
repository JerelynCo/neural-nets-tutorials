{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Neural Networks with Python - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is an Artificial Neural Network?\n",
    "\n",
    "> Neural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns. They interpret sensory data through a kind of machine perception, labeling or clustering raw input. The patterns they recognize are numerical, contained in vectors, into which all real-world data, be it images, sound, text or time series, must be translated.\n",
    " -- [deeplearning4j](https://deeplearning4j.org/neuralnet-overview)\n",
    "\n",
    "![Artificial Neural Network](figs/ann.png)\n",
    "\n",
    "An artificial neural network, or ANN, is __composed of a set of neurons, or perceptrons__, that are connected in a __hierarchical order__. In the context of __supervised learning__, where the objective is to __approximate a function given some data__, neural networks are theoretically able to approximate any function. Thus, neural networks are also referred to as __universal function approximators__. \n",
    "\n",
    "### Perceptrons\n",
    "\n",
    "Let's start with a single neuron first, also called the Perceptron.\n",
    "\n",
    "\n",
    "![Perceptron](figs/perceptron.png)\n",
    "\n",
    "A perceptron is a node that accepts a __fixed arbitrary number of inputs__, and gives a __single output__.\n",
    "\n",
    "\n",
    "The __importance of the inputs__ of a perceptron is represented by the __weights__ or visually, the strength of the edges from the inputs to the neurons.\n",
    "\n",
    "As the perceptron is biologically-inspired from how actual neurons work, if the values of the inputs satisfy a certain threshold then the perceptron gets turned on or \"activated\". This threshold value is commonly referred to as the __bias__.\n",
    "\n",
    "For now, let's just assume that the activation output of the perceptron is binary, meaning that its outputs are just 0 or 1.\n",
    "\n",
    "Mathematically, it's expressed as:\n",
    "\n",
    "![Perceptron Equation](figs/perceptron_eq.png)\n",
    "\n",
    "where $\\cdot$ is the dot product of the inputs and the weights.\n",
    "\n",
    "Generalizing the equation for the perceptron, \n",
    "\n",
    "$ output = \\alpha(w \\cdot x + b) $\n",
    "\n",
    "where $ \\alpha $ = 1 if greater than 0, else 0.\n",
    "\n",
    "$ \\alpha $ refers to the __activation__ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ther perceptron output is 0\n"
     ]
    }
   ],
   "source": [
    "# Code implementation of a simple perceptron\n",
    "\n",
    "w = [0.1, 0.2, 0.7]\n",
    "x = [1, 1, 0]\n",
    "b = -0.4\n",
    "\n",
    "def activation(val):\n",
    "    if val <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "output = activation(np.dot(w, x) + b)\n",
    "print(\"Ther perceptron output is %d\" % output)\n",
    "\n",
    "# Exercise:\n",
    "# Can you create an activation function that results to a perceptron acting as a linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary activation function of the perceptron is a bit limiting. What if we wanted outputs that are not just 0s or 1s? \n",
    "\n",
    "One available activation function that we can use is the sigmoid $\\sigma$ or logistic function.\n",
    "\n",
    "![Sigmoid function](figs/sigmoid_eq.png)\n",
    "\n",
    "It's good to note that there are a lot of activation functions that are being used in neural networks. ReLu, ELU, SeLU, and a whole lot of these functions greatly determine the performance of neural networks. Finding good activation functions are still an active area of research in this field.\n",
    "\n",
    "In fact, the sigmoid or logistic function isn't used that often as it encounters the __vanishing gradient problem__, which will be discussed later on. \n",
    "\n",
    "But for now, it's a great starting point for learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17309c4b550>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHxVJREFUeJzt3Xl4lOW9//H3l6wkIWxJWBP2xciiGAH1KFhLRdRitXXB\npWhb6vbTtmrV2mN76tLF2mp/LlQ9uKLU1o0iKtW6VQUJyBbWsCUsgYTsCVkmc58/Er0ighlgkmeW\nz+u65prMzEPmM4qf6/ae+3luc84hIiKRpZPXAUREJPhU7iIiEUjlLiISgVTuIiIRSOUuIhKBVO4i\nIhFI5S4iEoFU7iIiEUjlLiISgWK9euO0tDQ3cOBAr95eRCQsLVu2rMQ5l97WcZ6V+8CBA8nNzfXq\n7UVEwpKZbQ/kOE3LiIhEIJW7iEgEUrmLiESgNsvdzOaY2V4zW3OI183M/mJm+Wa2yszGBT+miIgc\njkBG7k8BU7/m9bOAYS23WcCjRx9LRESORpvl7pz7ACj9mkOmA8+4ZouBbmbWJ1gBRUTk8AVjzr0f\nUNjq8Y6W50RExCMdus7dzGbRPHVDVlZWR761iEi7q/c1UVXna7k1Ul3vo7a+iZoGHzX1TdTU+6hp\n8DEuqzunDW/zPKSjEoxy3wlktnrcv+W5r3DOPQY8BpCTk6PNW0UkJDnnKK9tZF9NPcVVDZRU11NW\n20BZTSPl+xsor22kvLaBstpGKvY3UlXXSGWdjwafP6Dff83kIWFR7vOB681sHjABqHDO7Q7C7xUR\nCbp6XxM7y/azu6KOooo6iiqb73dX1FFUuZ/iqnr2VTfg8x98/NklIZauSXF0T4qnW1Ic/bt3JrVz\nHF0SY0lNbL7vkhhLl4Q4khNiSUmIJSkhhuT4WJITYkiKjyWmk7X752yz3M3sBWAykGZmO4BfAXEA\nzrnZwEJgGpAP1AJXtldYEZFAVNf72Ly3mi0l1RTs209BaS2FpbUUltVSVFmHO6C3uyXF0Ts1kd5d\nE8nuk0paSgI9UxJIS4knveXnHsnNZR4XEx6nB7VZ7s65S9p43QHXBS2RiEiAaup9rNtdybqiKjbv\nrSa/5VZUWffFMWbQq0siWT2SOHlIGpk9OpPZPYm+3TrTp2tzoSfGxXj4KdqHZxcOExE5HBX7G1m9\no4K8XRWs2VVJ3q4KtpbUfDEKT46PYWhGCicP6cmQjBSGZqQwJD2Z/t2TIrK826JyF5GQ45xjZ/l+\ncreVkbu9lNxtZWzYU/VFkffr1pnsvqlMH9uPY/umkt03lT5dEzFr/7nscKFyF5GQUFxVz4ebivlg\nYzFLtpayu6J5aiUlIZbjs7oxbXQfjs/qxqi+XemeHO9x2tCnchcRTzQ2+fmsoJz3N+7l/Y3FrNlZ\nCUBaSjwTB/fkxIE9yBnYnZG9UztkdUmkUbmLSIdpbPLzUX4JC1btZlFeEZV1PmI6GSdkdeeWM0cw\naXg62X1S6aQyP2oqdxFpV74mP4u3lLJg1S7ezCuivLaRLgmxTDm2F1OO6cXJQ9Po2jnO65gRR+Uu\nIu1ia0kNL3xawMvLd1BS3UByfAxTsntx9pi+nDY8jYTY6FvB0pFU7iISNI1Nft5eu4e5Swr4T34J\nMZ2MKcf04rzj+zJ5REZULkn0ispdRI5aUUUdzy3ezt9yCymuqqdft87c/K3hXJiTSUZqotfxopLK\nXUSOWGFpLY++v5l/5O7A5/fzjZEZzJiQxaThGVrh4jGVu4gctvy91TzyXj6vrdhFjBnfy+nP1ZOG\nkNkjyeto0kLlLiIBW19UyV/e2cQba4pIjI1h5skDmXXaYHpp6iXkqNxFpE0l1fXcv2gjf1taQHJ8\nLNdOHsJVpwyiZ0qC19HkEFTuInJI9b4mnvpoGw/9O5/9jU3MPHkQN5wxlG5JOv0/1KncReQrnHO8\nlbeHexeuo6C0lm+MzOCOs49hSHqK19EkQCp3EfmSwtJabnt5FR/l72NYRgpPXzWeSe28JZwEn8pd\nRADw+x1zPy3gtwvXEWPGb6Yfy4zxWcSGyc5D8mUqdxGhsLSWW19axceb93HqsDR+d8EY+nXr7HUs\nOQoqd5Eo5pzj+U8LuPf1dQDc+53RXDI+U5teRACVu0iU2ltVx00vruTDTSWcMrQnv79gDP276ySk\nSKFyF4lCudtKuXbucirrGrnrvFFcNiFLo/UIo3IXiSLOOZ7+eBt3v76Oft078/RV4zmmT6rXsaQd\nqNxFokRtg49fvLyaV1fs4pvHZHD/hcdpk4wIpnIXiQJbS2q4+tllbNxbxc3fGs61k4dqK7sIp3IX\niXAfbCzmurnLiY0xnr5yPKfphKSooHIXiWCvfraTm/++kmG9uvD4FSdoNUwUUbmLRKjHP9jCPQvX\ncdLgnvz1ihNITdT8ejRRuYtEGL/fce/CdTzxn62cPaYPf7pwrDajjkIqd5EI0uDzc8s/VvLail3M\nPHkgd56TrS9Oo5TKXSRCVNf7uOa5ZXy4qYSfTx3BNZOG6MSkKKZyF4kA1fU+vj/nU1YUlnPfd8fw\nvZxMryOJxwK6lqeZTTWzDWaWb2a3HeT1rmb2TzNbaWZ5ZnZl8KOKyMHUNvi46qmlrCgs5+EZx6vY\nBQig3M0sBngYOAvIBi4xs+wDDrsOWOucGwtMBu43M+3DJdLO6hqb+NEzueRuK+XPFx3H1FF9vI4k\nISKQkft4IN85t8U51wDMA6YfcIwDuljzBF8KUAr4gppURL6k3tfEj59dxseb9/HH743l22P7eh1J\nQkgg5d4PKGz1eEfLc609BBwD7AJWAzc65/xBSSgiX9Hg83Pd3OW8v7GY350/mvPH9fc6koSYYO2f\ndSawAugLHAc8ZGZfudScmc0ys1wzyy0uLg7SW4tEF1+Tnxvnfcbb6/Zy13mjuOjELK8jSQgKpNx3\nAq2/oenf8lxrVwIvu2b5wFZg5IG/yDn3mHMuxzmXk56u61uIHC6/33HLP1bxxpoi7jwnm8snDvA6\nkoSoQMp9KTDMzAa1fEl6MTD/gGMKgDMAzKwXMALYEsygIgJ/+tdGXvlsJ7ecOYKr/muQ13EkhLW5\nzt055zOz64G3gBhgjnMuz8yubnl9NnAX8JSZrQYMuNU5V9KOuUWizt+WFvDQu/lcMj6TaycP8TqO\nhLiATmJyzi0EFh7w3OxWP+8CvhXcaCLyuQ83FfOLV9Zw2vB0fjN9lM48lTYF6wtVEWkn64squfa5\n5QzLSOHhGccTF6P/bKVt+lsiEsL2VNZx1ZNLSUqIYc7ME+miy/ZKgHRtGZEQVVPv4wdPL6V8fyMv\n/vgk+nbr7HUkCSMauYuEIL/fceO8z1i7q5KHZ4xjVL+uXkeSMKNyFwlBD76zibfX7eVX5x7L6SMz\nvI4jYUjlLhJi/r1+Dw++s4nvntCfK07SSUpyZFTuIiGkYF8tP5m3guw+qdx9npY8ypFTuYuEiP0N\nTfz4uWUAzL7sBBLjtO+pHDmtlhEJAc45fvnqGtbtruTJmSeS1TPJ60gS5jRyFwkBz39awEvLd3Dj\nGcP0BaoEhcpdxGOfFZTx6/l5TBqezo1nDPM6jkQIlbuIh0prGrh27nJ6pSby4MXH0amTvkCV4NCc\nu4hHnHPc+tIq9lU38NI1J9MtSdsOS/Bo5C7ikec/LeBfa/fw86kjGN1fZ6BKcKncRTyQv7eKuxas\n5dRhaVx1ijbdkOBTuYt0sHpfEze8sIKk+Fju/95YzbNLu9Ccu0gH++NbG1i7u5LHr8ghIzXR6zgS\noTRyF+lAH24q5vEPt3LZxCymZPfyOo5EMJW7SAcprWngphdXMjQjhTumZXsdRyKcyl2kAzjn+Pk/\nVlFe28hfLj6ezvG6boy0L5W7SAeYt7SQt9c1L3vM7pvqdRyJAip3kXa2o6yWuxes5eQhPbXsUTqM\nyl2kHTnnuO2l1QD8/oIxWvYoHUblLtKOXvi0kP/kl3D7tGPI7KHL+ErHUbmLtJMdZbXc83rzdMyl\nE7K8jiNRRuUu0g4OnI7RdnnS0VTuIu1A0zHiNZW7SJBpOkZCgcpdJIg0HSOhQuUuEkTzlmo6RkKD\nyl0kSPZU1nHv6+s4abCmY8R7AZW7mU01sw1mlm9mtx3imMlmtsLM8szs/eDGFAl9v56fR0OTn9+e\nP1rTMeK5Nq/nbmYxwMPAFGAHsNTM5jvn1rY6phvwCDDVOVdgZhntFVgkFC3KK+KNNUXccuYIBqYl\nex1HJKCR+3gg3zm3xTnXAMwDph9wzAzgZedcAYBzbm9wY4qErqq6Ru58LY+Rvbsw67TBXscRAQIr\n935AYavHO1qea2040N3M3jOzZWZ2RbACioS6+xdtZE9VHb89fzRxMfoaS0JDsLbZiwVOAM4AOgOf\nmNli59zG1geZ2SxgFkBWlr5wkvC3vKCMpz/ZxvdPGsjxWd29jiPyhUCGGTuBzFaP+7c819oO4C3n\nXI1zrgT4ABh74C9yzj3mnMtxzuWkp6cfaWaRkNDY5Of2l1bTOzWRm88c4XUckS8JpNyXAsPMbJCZ\nxQMXA/MPOOY14L/MLNbMkoAJwLrgRhUJLY99sIUNe6r4zfRRpCRor3kJLW3+jXTO+czseuAtIAaY\n45zLM7OrW16f7ZxbZ2ZvAqsAP/CEc25NewYX8dLWkhoefGcT00b31kbXEpICGm445xYCCw94bvYB\nj+8D7gteNJHQ5JzjjldWkxDbiV+fe6zXcUQOSl/tixym11bs4uPN+/j51JFkpCZ6HUfkoFTuIoeh\noraRu19fy9jMblw6Xiu+JHSp3EUOw32L1lNa08A9543SfqgS0lTuIgFaUVjO3CUFfP/kgYzq19Xr\nOCJfS+UuEgBfk587XllNRpcEfjZluNdxRNqkchcJwLOLt5O3q5I7zzmWLolxXscRaZPKXaQNeyrr\nuH/RRk4bns600b29jiMSEJW7SBvuWrCWhiY/d00/Vtdpl7Chchf5Gh9sLGbBqt1cf/pQBvTUddol\nfKjcRQ6hrrGJO19bw+C0ZH48Sddpl/Ciqx2JHMLs9zezbV8tc384gYTYGK/jiBwWjdxFDmL7vhoe\neW8z547tyylD07yOI3LYVO4iB3DOcedrecTHdOKXZx/jdRyRI6JyFznAW3lFvL+xmJ9NGU4vXRhM\nwpTKXaSVmnofv/nnWo7pk8oVJw3wOo7IEVO5i7Tyl39vYldFHXefdyyx2uxawpj+9oq02LSniv/9\ncCsX5WRywoAeXscROSoqdxGav0T95atrSEmM5dazRnodR+SoqdxFgFdX7GTJ1lJunTqSHsnxXscR\nOWoqd4l6Ffsbuef19RyX2Y2LcjK9jiMSFDpDVaLe/Ys2UFpTz1NXnqjdlSRiaOQuUW1lYTnPLt7O\nFSdpdyWJLCp3iVpNfscdr64mPSWBm76l3ZUksqjcJWo9+8k21uys5M5zs7W7kkQclbtEpb0tuyud\nOiyNs0f38TqOSNCp3CUq3fX6Ouqb/Nw1fZR2V5KIpHKXqPPhpmL+uXIX100eysA07a4kkUnlLlGl\nrrGJ/351DYPSkrl6snZXksilde4SVR59r3l3ped+oN2VJLJp5C5RY2tJDY++t5lvj+3Lfw3T7koS\n2VTuEhWcc9zxymoSYrW7kkSHgMrdzKaa2QYzyzez277muBPNzGdm3w1eRJGj99LynXy8eR+3TRtJ\nhnZXkijQZrmbWQzwMHAWkA1cYmbZhzju98CiYIcUORol1fXc/fpacgZ055ITs7yOI9IhAhm5jwfy\nnXNbnHMNwDxg+kGO+3/AS8DeIOYTOWp3L1hLTb2P354/WhcGk6gRSLn3AwpbPd7R8twXzKwf8B3g\n0eBFEzl6728s5tUVu7hm8lCG9eridRyRDhOsL1QfAG51zvm/7iAzm2VmuWaWW1xcHKS3Fjm42gYf\nd7yymsHpyVx3+hCv44h0qEDWue8EWu9g0L/ludZygHktp3GnAdPMzOece7X1Qc65x4DHAHJyctyR\nhhYJxANvb2JH2X7+Nmui1rRL1Amk3JcCw8xsEM2lfjEwo/UBzrlBn/9sZk8BCw4sdpGOtGZnBU98\nuIVLxmcyYXBPr+OIdLg2y9055zOz64G3gBhgjnMuz8yubnl9djtnFDksviY/t7+8mh7JCdw2VWva\nJToFdPkB59xCYOEBzx201J1zM48+lsiRe+rjbazeWcHDM8bRNUnXaZfopDNUJaJsLanhj4s2cMbI\nDKaN7u11HBHPqNwlYjT5Hbf8fSXxMZ249/zRuk67RDWVu0SMpz7eRu72Mn517rH00iUGJMqp3CUi\nbC2p4b631nPGyAzOH9ev7T8gEuFU7hL2NB0j8lUqdwl7mo4R+SqVu4Q1TceIHJzKXcKWpmNEDk3l\nLmFL0zEih6Zyl7C0cU8Vf3hT0zEih6Jyl7BT19jEDS98RpfEWH53wRhNx4gcREDXlhEJJX94cwPr\ni6p4cuaJpHdJ8DqOSEjSyF3Cynsb9jLno618/6QBnD4yw+s4IiFL5S5ho6S6npv/vooRvbpw+zRd\nylfk62haRsKCc83LHivrGnnuh+NJjNPOSiJfRyN3CQvPfLKddzcUc/tZIxnZO9XrOCIhT+UuIW9D\nURX3LFzH5BHpzDx5oNdxRMKCyl1CWl1jEzfO+4zUxFju++5YLXsUCZDm3CVkOee445U1zcser9Sy\nR5HDoZG7hKznPy3gpeU7uOGMYZw+QsseRQ6Hyl1C0orCcv5n/lomDU/nxjOGeR1HJOyo3CXk7Kuu\n59rnlpHeJYEHLjqOmE6aZxc5XJpzl5DS5HfcMO8zSmoaeOnqk+meHO91JJGwpJG7hJT7F23go/x9\n3D19FKP7d/U6jkjYUrlLyFiUV8Qj723mkvGZXHhiptdxRMKayl1Cwubiam56cSVj+nflV+ce63Uc\nkbCnchfP7auu58onlxIf24lHLh2n68aIBIG+UBVP1TU28cNnctlTWce8WRPp3z3J60giEUHlLp7x\n+x0/e3EFKwrLeWTGOI7P6u51JJGIoWkZ8czv31rPwtVF/OKsYzhrdB+v44hEFJW7eGLuku389f0t\nXD5xAD88dZDXcUQijspdOty7G/Zy52t5nD4inV+dm60rPYq0g4DK3cymmtkGM8s3s9sO8vqlZrbK\nzFab2cdmNjb4USUS5O2q4Pq5yxnZuwsPzRhHbIzGFyLtoc3/sswsBngYOAvIBi4xs+wDDtsKTHLO\njQbuAh4LdlAJfxv3VHH5/35K185xzJl5IskJ+j5fpL0EMmwaD+Q757Y45xqAecD01gc45z52zpW1\nPFwM9A9uTAl3m4urmfH4EmI7Gc//aCK9UhO9jiQS0QIp935AYavHO1qeO5QfAG8c7AUzm2VmuWaW\nW1xcHHhKCWvbSmqY8fhiwPH8jyYyMC3Z60giES+oE55mdjrN5X7rwV53zj3mnMtxzuWkp6cH860l\nRBWW1jLj8cU0+PzM/eFEhmakeB1JJCoEMum5E2h9Faf+Lc99iZmNAZ4AznLO7QtOPAlnu8r3M+OJ\nxVTX+3j+RxMZ0buL15FEokYgI/elwDAzG2Rm8cDFwPzWB5hZFvAycLlzbmPwY0q42VNZx4zHF1Ne\n08izP5jAqH66fK9IR2pz5O6c85nZ9cBbQAwwxzmXZ2ZXt7w+G7gT6Ak80rJm2eecy2m/2BLKCvbV\ncsWcJRRX1fPMD8YzNrOb15FEoo455zx545ycHJebm+vJe0v7WbOzgplPLqWxyc+cmTmcMKCH15FE\nIoqZLQtk8KyFxhI0H+WX8ONnl5GaGMu8WScxNENz7CJeUblLUMxfuYubXlzB4LQUnr5qPL27ah27\niJdU7nLU5vxnK79ZsJbxA3vw+BU5dE2K8zqSSNRTucsRa/I7/vDmev76wRamHtubBy4+TrsoiYQI\nlbsckbKaBm6Y9xkfbirhsolZ/M+3RxHTSVd3FAkVKnc5bKt2lHPNc8sprqrnd+eP5uLxWV5HEpED\nqNzlsPxtaQH//Voe6SkJ/P3qk7SGXSREqdwlIHWNTfx6fh7zlhZy6rA0Hrz4eHokx3sdS0QOQeUu\nbdpcXM1P5q1g9c4Krj99KD+dMlzz6yIhTuUuh9Tkd8z5z1b+uGgDneNjePyKHKZk9/I6logEQOUu\nB7WluJpb/rGKZdvLmJLdi3u+M4qMLjoxSSRcqNzlS/x+x5Mfb+MPb64nIbYTf75oLOcd10+bWIuE\nGZW7fCF/bxW/eHkNn24r5YyRGdx7/mhthycSplTuQllNAw+8vZHnlhSQHB/DH783lgvGabQuEs5U\n7lGsscnPs59s58F3NlFV18ilEwbw0ynDtcRRJAKo3KOQc473NhRz1+tr2VJcw6nD0vjl2dnaBk8k\ngqjco4hzjo/y9/HQu5tYvKWUwWnJzJmZw+kjMjQFIxJhVO5RwDnHO+v28v/fzWdlYTm9UhP49bnZ\nzJgwgPjYQLbRFZFwo3KPYE1+x8LVu3n43XzWF1WR2aMz935nNBec0I+EWF2aVySSqdwj0L7qev6+\nbAfPLymgoLSWIenJ/OnCsXx7bF9iYzRSF4kGKvcI4ZxjydZS5i4p4M01u2lscowf1IPbzhrJmcf2\n1rVgRKKMyj3MFVXUsWDVLl74tIDNxTWkJsZy2cQBXDohSxtUi0QxlXsYKq6q5401u1mwcjdLt5fi\nHByX2Y37vjuGc8b0pXO85tNFop3KPUzsKt/Pv9fv5fVVu1mydR9+ByN6deGn3xzO2WP6MCQ9xeuI\nIhJCVO4hqt7XxNKtZby/cS/vbyxm455qAAanJ3P9N4Zxzpg+DO+laRcROTiVe4ho8PlZs6uC3G2l\nLN5Syieb97G/sYn4mE5MGNyDC3MymTQ8naEZKTrhSETapHL3SFlNAyt2lJO7rZSl28pYWVhOvc8P\nwKC0ZC7M6c+kEelMHNyTpHj9axKRw6PWaGfOOfZU1rNmZwV5uyrJ29V8v7N8PwAxnYxRfVO5bOIA\nThzYnXEDumtTDBE5air3IPH7HTvL95NfXM3mvdXk761mc3HzfVltIwBmzaPycQO6c/lJAxjTryvH\nZXXTyFxEgk6tEiDnHBX7G9lVXkdhWS2Fpc23gtJaCsv2U1ha+8W0CkCP5HiGpqcwdVQfRvRKYVS/\nrhzTJ5XkBP0jF5H2F/VN0+R3lNU2sK+6gZLq+pZbA8VV9RRV7Keoso6iijp2V9R9qbwBuiTEktkj\niaHpKZw+Ip3B6SkMzUhhSHqKrokuIp4KqNzNbCrwIBADPOGc+90Br1vL69OAWmCmc255kLMelHOO\nep+fmnofNfVNVNY1UlXno+qA+/L9jZTXNlJe20BZbcOXHvvdV39vXIzRKzWRPl0TGd2/G1OyE+jd\ntTN9uiaS2T2JzB6d6do5TitXRCQktVnuZhYDPAxMAXYAS81svnNubavDzgKGtdwmAI+23Afdexv2\ncteCtdTUN1HT4KO2oYmmg7XzAZLjY+iWFE/XznF0T46jT9fOdEuKo0dyPGkpCfRMab5vvsWruEUk\nrAUych8P5DvntgCY2TxgOtC63KcDzzjnHLDYzLqZWR/n3O5gB07tHMfI3qkkJ8SQFB9LSkIsSQkx\nzffxsXRJbL6lJsa1/Nx8H6erIYpIFAmk3PsBha0e7+Cro/KDHdMP+FK5m9ksYBZAVlbW4WYFYFxW\nd8Zd2v2I/qyISLTo0OGsc+4x51yOcy4nPT29I99aRCSqBFLuO4HMVo/7tzx3uMeIiEgHCaTclwLD\nzGyQmcUDFwPzDzhmPnCFNZsIVLTHfLuIiASmzTl355zPzK4H3qJ5KeQc51yemV3d8vpsYCHNyyDz\naV4KeWX7RRYRkbYEtM7dObeQ5gJv/dzsVj874LrgRhMRkSOl9YEiIhFI5S4iEoFU7iIiEciap8s9\neGOzYmC7J29+dNKAEq9DdDB95sgXbZ8XwvczD3DOtXmikGflHq7MLNc5l+N1jo6kzxz5ou3zQuR/\nZk3LiIhEIJW7iEgEUrkfvse8DuABfebIF22fFyL8M2vOXUQkAmnkLiISgVTuR8HMbjIzZ2ZpXmdp\nT2Z2n5mtN7NVZvaKmXXzOlN7MbOpZrbBzPLN7Dav87Q3M8s0s3fNbK2Z5ZnZjV5n6ihmFmNmn5nZ\nAq+ztAeV+xEys0zgW0CB11k6wL+AUc65McBG4HaP87SLVltKngVkA5eYWba3qdqdD7jJOZcNTASu\ni4LP/LkbgXVeh2gvKvcj92fg50DEf2nhnFvknPO1PFxM8/X6I9EXW0o65xqAz7eUjFjOud2fb2bv\nnKuiuez6eZuq/ZlZf+Bs4Amvs7QXlfsRMLPpwE7n3Eqvs3jgKuANr0O0k0NtFxkVzGwgcDywxNsk\nHeIBmgdnfq+DtJeALvkbjczsbaD3QV66A/gFzVMyEePrPq9z7rWWY+6g+X/j53ZkNml/ZpYCvAT8\nxDlX6XWe9mRm5wB7nXPLzGyy13nai8r9EJxz3zzY82Y2GhgErDQzaJ6iWG5m451zRR0YMagO9Xk/\nZ2YzgXOAM1zkrp+Nyu0izSyO5mKf65x72es8HeAU4NtmNg1IBFLN7Dnn3GUe5woqrXM/Sma2Dchx\nzoXjBYgCYmZTgT8Bk5xzxV7naS9mFkvzF8Zn0FzqS4EZzrk8T4O1I2seoTwNlDrnfuJ1no7WMnK/\n2Tl3jtdZgk1z7hKIh4AuwL/MbIWZzW7rD4Sjli+NP99Sch3wYiQXe4tTgMuBb7T8u13RMqKVMKeR\nu4hIBNLIXUQkAqncRUQikMpdRCQCqdxFRCKQyl1EJAKp3EVEIpDKXUQkAqncRUQi0P8BUpQWrh4n\n4s8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17307bc8940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(val):\n",
    "    return 1 / (1 + np.exp(-val))\n",
    "\n",
    "w = [1]\n",
    "xs = np.linspace(-5, 5)\n",
    "b = 0\n",
    "outputs = []\n",
    "for x in xs:\n",
    "    outputs.append(sigmoid(np.dot(w, x) + b))\n",
    "\n",
    "plot(xs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now what? How does a perceptron or neural networks even _learn_?__\n",
    "\n",
    "You might have noticed that the __weights and the biases__ of a perceptron heavily __influences its output__, and you're right! \n",
    "\n",
    "Recall that the objective is to __approximate a function given some data__. \n",
    "\n",
    "Thus, if we give __a set of inputs and outputs__ for our perceptron, it should be able to __adjust its weights and bias accordingly__. \n",
    "\n",
    "How then, can the perceptron know that it needs to improve and change its weights and biases? \n",
    "\n",
    "### The Cost function\n",
    "Also referred to as the objective function or the loss function.\n",
    "\n",
    "The mission of the cost function is __to quantify__ and tell the perceptron on how __good or accurate__ it is when given some inputs and target outputs.\n",
    "\n",
    "Like the activation function, there are a variety of cost functions that we can use. For now, let's stick with the __Mean Squared Error__ or __MSE__.\n",
    "\n",
    "Mathematically, it's defined as:\n",
    "\n",
    "$ C(w, b) = \\frac{1}{n} \\sum_x (y_p(x) - y_t)^2 $\n",
    "\n",
    "Where: \n",
    "\n",
    "__w__ is the weights\n",
    "\n",
    "__b__ is the bias\n",
    "\n",
    "__n__ is the number of data points\n",
    "\n",
    "__$y_p$__ is the predicted output of the perceptron\n",
    "\n",
    "__x__ input data\n",
    "\n",
    "__$y_t$__ is the real output\n",
    "\n",
    "The properties of the MSE function is as follows:\n",
    "\n",
    "1. It is always positive, because of the squaring of the differences between the predicted and target outputs.\n",
    "2. When the difference between the predicted and target outputs are high, then the value of the cost function is also high.\n",
    "3. If the predicted and target outputs are the same, then the cost function will be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementation of the Mean Squared Error\n",
    "\n",
    "def sigmoid(val):\n",
    "    return 1 / (1 + np.exp(-val))\n",
    "\n",
    "def perceptron(w, b, x):\n",
    "    return sigmoid(np.dot(w, x) + b)\n",
    "\n",
    "def cost_function(w, b, xs, yts):\n",
    "    n = len(xs)\n",
    "    squared_err = np.array([])\n",
    "    for x, yt in zip(xs, yts):\n",
    "        squared_err = np.append(squared_err, np.square(perceptron(w, b, x) - yt))\n",
    "    return squared_err.sum() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated cost is  0.13426267352\n"
     ]
    }
   ],
   "source": [
    "# Example calculation of cost function\n",
    "\n",
    "w = [0.5, 0.3, 0.8]\n",
    "b = -0.4\n",
    "xs = [[1, 0, 1], \n",
    "      [1, 1, 1],\n",
    "      [0, 1, 0], \n",
    "      [0, 1, 1], \n",
    "      [1, 0, 0]]\n",
    "yts = [0.5, 0.2, 0.8, 0.8, 0.1]\n",
    "\n",
    "print(\"Calculated cost is \", cost_function(w, b, xs, yts)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now that the perceptron \"knows\" how good its weights and bias are, how can it adapt and attempt reduce the cost function to zero?__\n",
    "\n",
    "### Optimization Algorithms\n",
    "The goal of optimization algorithms is __to reduce the cost function__ to its minimum value. \n",
    "\n",
    "Like activation and cost functions, there are also a variety of different optimization algorithms. For neural networks, the popular and mostly successful one is a family of algorithms called __Gradient Descent__.\n",
    "\n",
    "An ELI5 or Explain like I'm 5 for gradient descent is:\n",
    "\n",
    "> Imagine you're standing somewhere on a mountain. You want to get as low as possible as fast as possible, so you decide to follow these steps:\n",
    "You check your current altitude, your altitude a step north, a step south, a step east, and a step west. Using this, you figure out which direction you should step to reduce your altitude as much as possible in this step.\n",
    "Repeat until stepping in any direction will cause you to go up again.\n",
    "Now, this is a wonderfully simple algorithm. You just have to figure out which direction the mountain is sloping where you are (this is the gradient) and take a step (this is your descent). -- [koooooj](https://www.reddit.com/r/explainlikeimfive/comments/2akok1/eli5_what_is_gradient_descent/ciw488c/) \n",
    "\n",
    "Given the possible outputs of a cost function of a 2-input perceptron is the graph below.\n",
    "\n",
    "![Cost graph](figs/cost_graph.png)\n",
    "\n",
    "Mathematically, in the context of our perceptron:\n",
    "\n",
    "![Gradient Descent](figs/grad_descent_eq.png)\n",
    "\n",
    "Where:\n",
    "\n",
    "$ \\eta $ is the learning rate or how large the step of the gradient descent will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GD_training():\n",
    "    w = np.array([0.5, 0.3, 0.8])\n",
    "    b = -0.4\n",
    "    xs = np.array([[1, 0, 1], \n",
    "          [1, 1, 1],\n",
    "          [0, 1, 0], \n",
    "          [0, 1, 1], \n",
    "          [1, 0, 0]])\n",
    "    yts = [0.5, 0.2, 0.8, 0.8, 0.1]\n",
    "    eta = 0.01\n",
    "    for i in range(200):\n",
    "        cost = cost_function(w, b, xs, yts)\n",
    "        w -= eta * cost\n",
    "        b -= eta * cost\n",
    "        if i % 10 == 0:\n",
    "            print(\"Step \", i, \": \", cost)\n",
    "    print(\"Calculated weights: \", w)\n",
    "    print(\"Calculated bias: \", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  0 :  0.13426267352\n",
      "Step  10 :  0.131649082408\n",
      "Step  20 :  0.129149946578\n",
      "Step  30 :  0.126765301758\n",
      "Step  40 :  0.124494919158\n",
      "Step  50 :  0.122338339541\n",
      "Step  60 :  0.12029490698\n",
      "Step  70 :  0.11836380186\n",
      "Step  80 :  0.116544072756\n",
      "Step  90 :  0.114834666907\n",
      "Step  100 :  0.113234459065\n",
      "Step  110 :  0.111742278571\n",
      "Step  120 :  0.110356934567\n",
      "Step  130 :  0.109077239293\n",
      "Step  140 :  0.107902029477\n",
      "Step  150 :  0.106830185839\n",
      "Step  160 :  0.105860650771\n",
      "Step  170 :  0.104992444258\n",
      "Step  180 :  0.104224678142\n",
      "Step  190 :  0.103556568801\n",
      "Calculated weights:  [ 0.26977761  0.06977761  0.56977761]\n",
      "Calculated bias:  -0.630222387699\n"
     ]
    }
   ],
   "source": [
    "GD_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "1. Try to use other cost functions, such as RMSE or MAE. How does it affect the gradient descent process?\n",
    "2. What about activation functions? Does the sigmoid function help or not? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Week\n",
    "\n",
    "1st Day:\n",
    "1. We'll be creating our own multilayer perceptrons (MLPs)\n",
    "2. We'll be discussing about how backpropagation works\n",
    "\n",
    "2nd Day:\n",
    "1. We'll fast track to using Tensorflow and its various APIs.\n",
    "2. We'll be discussing about Convolutional Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits to\n",
    "[Michael Nielsen: Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)\n",
    "for the Concepts, figures, and graphs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
